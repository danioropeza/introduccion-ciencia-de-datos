{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy = True\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43220</th>\n",
       "      <td>yeah right. Sammo Hung already acted in the ma...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32323</th>\n",
       "      <td>Loaded with fine actors, I expected much more ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11811</th>\n",
       "      <td>Although allegedly autobiographical, this movi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41119</th>\n",
       "      <td>My personal opinion is that this movie had no ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Is it possible for a movie to get any worse th...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34950</th>\n",
       "      <td>The same night that I watched this I also watc...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>Found this film in a DVD discount rack for $10...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35715</th>\n",
       "      <td>- A film crew is shooting a horror movie in an...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9713</th>\n",
       "      <td>After spending half an hour examining Rumors, ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45555</th>\n",
       "      <td>I came in in the middle of this film so I had ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review Label\n",
       "43220  yeah right. Sammo Hung already acted in the ma...   pos\n",
       "32323  Loaded with fine actors, I expected much more ...   neg\n",
       "11811  Although allegedly autobiographical, this movi...   neg\n",
       "41119  My personal opinion is that this movie had no ...   pos\n",
       "280    Is it possible for a movie to get any worse th...   neg\n",
       "...                                                  ...   ...\n",
       "34950  The same night that I watched this I also watc...   neg\n",
       "347    Found this film in a DVD discount rack for $10...   neg\n",
       "35715  - A film crew is shooting a horror movie in an...   neg\n",
       "9713   After spending half an hour examining Rumors, ...   neg\n",
       "45555  I came in in the middle of this film so I had ...   pos\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "ruta_archivo = os.path.join(\"imdb_dataset.csv\")\n",
    "df_criticas = pd.read_csv(ruta_archivo, encoding='iso-8859-2').sample(2000, replace=False)\n",
    "#df_criticas = pd.read_csv(ruta_archivo, encoding='iso-8859-2')\n",
    "df_criticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras de parada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '..',\n",
       " '...',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "palabras_de_parada_ingles = set(nltk.corpus.stopwords.words('english') + list(string.punctuation) + ['...', '..'])\n",
    "palabras_de_parada_ingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43220</th>\n",
       "      <td>yeah right sammo hung already acted main role ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32323</th>\n",
       "      <td>loaded fine actors expected much deceiver deli...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11811</th>\n",
       "      <td>although allegedly autobiographical movie demo...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41119</th>\n",
       "      <td>personal opinion movie real story line first j...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>possible movie get worse bunch apes wandering ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34950</th>\n",
       "      <td>night watched also watched scary movie 4 makin...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>found film dvd discount rack 10 worth camera w...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35715</th>\n",
       "      <td>film crew shooting horror movie old supposedly...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9713</th>\n",
       "      <td>spending half hour examining rumors gay bar lo...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45555</th>\n",
       "      <td>came middle film idea credits even title till ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review Label\n",
       "43220  yeah right sammo hung already acted main role ...   pos\n",
       "32323  loaded fine actors expected much deceiver deli...   neg\n",
       "11811  although allegedly autobiographical movie demo...   neg\n",
       "41119  personal opinion movie real story line first j...   pos\n",
       "280    possible movie get worse bunch apes wandering ...   neg\n",
       "...                                                  ...   ...\n",
       "34950  night watched also watched scary movie 4 makin...   neg\n",
       "347    found film dvd discount rack 10 worth camera w...   neg\n",
       "35715  film crew shooting horror movie old supposedly...   neg\n",
       "9713   spending half hour examining rumors gay bar lo...   neg\n",
       "45555  came middle film idea credits even title till ...   pos\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LimpiadorTexto es una clase que nosotros creamos.\n",
    "# Utilizamos la funcion normalizar aquÃ­, en el notebook. y tambiÃ©n en la API.\n",
    "# Por tanto, para no tener cÃ³digo repetido, abstraÃ­mos la lÃ³gica a una clase externa.\n",
    "from LimpiadorTexto import LimpiadorTexto\n",
    "limpiador = LimpiadorTexto(palabras_de_parada_ingles)\n",
    "df_criticas = df_criticas.apply(limpiador.normalizar_fila, axis=1)\n",
    "df_criticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener el vocabulario del problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_vocabulario_problema(df_criticas_normalizadas):\n",
    "    cuerpo_texto_normalizado = ' '.join(df_criticas_normalizadas['Review'].tolist())\n",
    "    vocabulario_problema = cuerpo_texto_normalizado.split()\n",
    "    vocabulario_ordenado = sorted(set(vocabulario_problema))\n",
    "    return vocabulario_ordenado\n",
    "\n",
    "def obtener_vocabulario_problema_y_posicion(vocabulario_del_problema_ordenado):\n",
    "    vocabulario_y_posicion = {}\n",
    "    for i, token in enumerate(vocabulario_del_problema_ordenado):\n",
    "        vocabulario_y_posicion[token] = i\n",
    "    return vocabulario_y_posicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29876"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulario_problema = obtener_vocabulario_problema_y_posicion(obtener_vocabulario_problema(df_criticas))\n",
    "len(vocabulario_problema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def obtener_one_hot_vector(critica, vocabulario_problema_y_posicion):\n",
    "    one_hot_vector = np.zeros(len(vocabulario_problema_y_posicion), dtype=int)\n",
    "    for token in critica.split():\n",
    "        one_hot_vector[vocabulario_problema_y_posicion[token]] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hots = []\n",
    "indices = []\n",
    "for indice, fila in df_criticas.iterrows():\n",
    "    one_hot = obtener_one_hot_vector(fila['Review'], vocabulario_problema)\n",
    "    indices.append(indice)\n",
    "    one_hots.append(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Crear y entrenar el modelo predictivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# se cambia el nombre de la variable 'one_hots' para mas claridad\n",
    "X = one_hots\n",
    "Y = df_criticas['Label'].ravel()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train =  np.array(X_train)\n",
    "X_test =  np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liberar la memoria RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "lst = [df_criticas]\n",
    "del df_criticas, one_hots, X, Y\n",
    "del lst\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar diferentes modelos con diferentes parÃ¡metros para encontrar el que produzca mejores resultados\n",
    "\n",
    "Se entrenarÃ¡n diferentes modelos con diferentes parÃ¡metros. Se guardarÃ¡ las mÃ©tricas de cada modelo en un archivo para que luego puedan ser comparadas, y asÃ­ determinar el mejor clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import json\n",
    "\n",
    "def guardar_objeto_json(nombre_objeto, objeto):\n",
    "    ruta_archivo = os.path.join(nombre_objeto)\n",
    "    with open(ruta_archivo, 'w') as fp:\n",
    "        json.dump(objeto, fp)\n",
    "    \n",
    "def obtener_metricas(clasificador):\n",
    "    predicciones = clasificador.predict(X_test)\n",
    "    metricas = {\n",
    "        'accuracy': accuracy_score(Y_test, predicciones),\n",
    "        'precision': precision_score(Y_test, predicciones, pos_label='pos'),\n",
    "        'recall': recall_score(Y_test, predicciones, pos_label='pos'),\n",
    "        'f1': f1_score(Y_test, predicciones, pos_label='pos')\n",
    "    }\n",
    "    return metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegresiÃ³n logÃ­stica con los parÃ¡metros:\n",
    "\n",
    "- random_state=42\n",
    "- solver='sag'\n",
    "- max_iter=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclasificador = LogisticRegression(random_state=0, solver='sag', max_iter=300)\\nclasificador.fit(X_train, Y_train)\\nmetricas = obtener_metricas(clasificador)\\nmetricas['model'] = 'LogisticRegression'\\nmetricas['solver'] = 'sag'\\nmetricas['random_state'] = 42\\nmetricas['max_iter'] = 300\\nguardar_objeto_json('metricas-reg-log1.txt', metricas)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "'''\n",
    "clasificador = LogisticRegression(random_state=0, solver='sag', max_iter=300)\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'LogisticRegression'\n",
    "metricas['solver'] = 'sag'\n",
    "metricas['random_state'] = 42\n",
    "metricas['max_iter'] = 300\n",
    "guardar_objeto_json('metricas-reg-log1.txt', metricas)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegresiÃ³n logÃ­stica con los parÃ¡metros:\n",
    "\n",
    "- random_state=42\n",
    "- solver='saga'\n",
    "- n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordanmontt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'guardar_objeto' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-70855e84555e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmetricas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'random_state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmetricas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_iter'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mguardar_objeto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metricas-reg-log2.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetricas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'guardar_objeto' is not defined"
     ]
    }
   ],
   "source": [
    "clasificador = LogisticRegression(random_state=42, solver='saga', max_iter=300, n_jobs=-1)\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'LogisticRegression'\n",
    "metricas['solver'] = 'saga'\n",
    "metricas['random_state'] = 42\n",
    "metricas['max_iter'] = 300\n",
    "guardar_objeto_json('metricas-reg-log2.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegresiÃ³n logÃ­stica con los parÃ¡metros:\n",
    "\n",
    "- random_state=42\n",
    "- solver='liblinear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = LogisticRegression(random_state=42, solver='liblinear')\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'LogisticRegression'\n",
    "metricas['solver'] = 'liblinear'\n",
    "metricas['random_state'] = 42\n",
    "guardar_objeto_json('metricas-reg-log3.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegresiÃ³n logÃ­stica con los parÃ¡metros:\n",
    "\n",
    "- solver='lbfgs'\n",
    "- n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = LogisticRegression(solver='lbfgs', n_jobs=-1)\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'LogisticRegression'\n",
    "metricas['solver'] = 'lbfgs'\n",
    "guardar_objeto_json('metricas-reg-log4.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegresiÃ³n logÃ­stica con los parÃ¡metros:\n",
    "\n",
    "- solver='lbfgs'\n",
    "- n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador = LogisticRegression(solver='newton-cg', n_jobs=-1)\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'LogisticRegression'\n",
    "metricas['solver'] = 'newton-cg'\n",
    "guardar_objeto_json('metricas-reg-log5.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees con optimizaciÃ³n de hiper-parÃ¡metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK,Trials\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "space =  {\n",
    "    'n_estimators':hp.quniform('n_estimators',100,1000,1),\n",
    "    'learning_rate':hp.quniform('learning_rate',0.025,0.5,0.025),\n",
    "    'max_depth':hp.quniform('max_depth',1,13,1),\n",
    "    'subsample': hp.quniform('subsample',0.5,1,0.05),\n",
    "    'colsample_bytree':hp.quniform('colsample_bytree',0.5,1,0.05),\n",
    "    'nthread':6, #cuando sea posible, paralelizar el procesamiento empleando hasta 6 hilos\n",
    "    'silent':1 # si occuriese un error, continuar con la ejecuciÃ³n\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['max_depth'] = int(params['max_depth'])  \n",
    "    clf = XGBClassifier(**params)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    accuracy = accuracy_score(Y_test, clf.predict(X_test))\n",
    "    return {'loss': 1 - accuracy, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "mejores_parametros = fmin(objective,space, algo=tpe.suggest, trials=trials, max_evals=100)\n",
    "mejores_parametros['n_estimators'] = int(mejores_parametros['n_estimators'])\n",
    "mejores_parametros['max_depth'] = int(mejores_parametros['max_depth']) \n",
    "\n",
    "clasificador = XGBClassifier(**mejores_parametros)\n",
    "clasificador.fit(X_train, Y_train)\n",
    "metricas = obtener_metricas(clasificador)\n",
    "metricas['model'] = 'GradientBoostedTrees'\n",
    "metricas['params'] = mejores_parametros\n",
    "guardar_objeto_json('metricas-GradientBoostedTrees.txt', metricas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el clasificador como un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "ruta_archivo_clasificador = os.path.join('clasificador-regresion-logistica.pkl')\n",
    "archivo_clasificador = open(ruta_archivo_clasificador, 'wb')\n",
    "pickle.dump(clasificador_reg_log, archivo_clasificador)\n",
    "archivo_clasificador.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el vocabulario del problema como un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo_vocabulario = os.path.join('vocabulario-problema.pkl')\n",
    "archivo_vacabulario = open(ruta_archivo_vocabulario, 'wb')\n",
    "pickle.dump(vocabulario_problema, archivo_vacabulario)\n",
    "archivo_vacabulario.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar las palabras de parada como un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo_palabras_parada = os.path.join('palabras-parada.pkl')\n",
    "archivo_palabras_parada = open(ruta_archivo_palabras_parada, 'wb')\n",
    "pickle.dump(palabras_de_parada_ingles, archivo_palabras_parada)\n",
    "archivo_palabras_parada.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
