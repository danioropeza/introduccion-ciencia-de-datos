{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Habilitar intellisense\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulario del problema\n",
    "En un proyecto de *NLP* se trabajan con colecciones de \"documentos\". Cada documento en un fragmento de texto que se debe procesar de manera individual (clasificar, representar, etc). Algunos ejemplos de documentos son: tweets, revisiones, artículos, libros, etc. El conjunto de todas la palabras que aparecen en todos los documentos constituye el **vocabulario del problema**. En muchos casos el tamaño del problema puede reducir el desempeño de la solución y es necesario reducirlo. \n",
    "\n",
    "Los elementos en el vocabulario se conocen como *tokens*; dependiendo del problema y las decisiones de diseño podrian no ser necesariamente una palabra. Por ejemplo, podrían ser emoticones, lemas o palabras compuestas.\n",
    "\n",
    "En este *notebook* se muestra como realizar una de las tareas más comunes que ayudan a reducir el tamaño del vocabulario: la eliminación de palabras de parada (*stop words*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Tokenización](#1)\n",
    "2. [Quitar palabras de parada](#2)\n",
    "3. [Otras tareas de limpieza que se puedan considerar](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## Tokenización\n",
    "[Tokenización](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html) es el proceso de separar el texto en piezas llamadas *tokens*. Es la primera tarea/proceso en cualquier proyecto *NLP*. Es fundamental realizarla bien para no afectar la calidad de los datos de entrada en las etapas siguientes. Se recomienda usar un *tokenizer* reconocido y evitar intentar programar uno desde cero.\n",
    "\n",
    "A continuación, se muestra un ejemplo de tokenización sobre esta [noticia](https://www.lostiempos.com/deportes/multideportivo/20200115/olympic-albert-einstein-ucb-lpz-van-paso-firme-liga-superior). Primero debes cargar la noticia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"Los clubes cochabambinos de Olympic, Albert Einstein y el paceño Universidad Católica Boliviana (UCB) avanzan a paso firme y constante rumbo a la corona en la Liga Superior de voleibol, rama femenina, que se desarrolla en el coliseo Julio Borelli Vitterito de La Paz, luego de cosechar sendas victorias la noche de este martes. El ganador será representante de Bolivia en la Liga Sudamericana de Clubes 2020.\n",
    "\n",
    "El campeón defensor del título, Olympic, superó 3-0  a su verdugo de la final de la edición 2017, el también cochabambino San Simón. La victoria para las olympiquistas fue con sets de 25-14, 25-13 y 25-18.\"\"\"\n",
    "texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización a nivel oraciones\n",
    "La función sent_tokenize(...) de nltk.tokenize es el tokenizador a nivel de oraciones. Si tienes problemas importanto la librería [*nltk*](https://www.nltk.org/api/nltk.tokenize.html), ejecuta nltk.download('punkt') desde la consola de python para descargar la librería."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "oracionesNoticia = nltk.tokenize.sent_tokenize(text=texto, language='spanish')\n",
    "print(\"Número total de oraciones de la noticia: {}\".format(len(oracionesNoticia)))\n",
    "print(\"Texto de la oracion 1: {}\".format(oracionesNoticia[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización de a nivel de palabras.\n",
    "La función nltk.word_tokenize(...) es el tokenizador a nivel de palabras recomendado por NLTK. El resultado de esta función es una lista con todas las palabras de la noticia. Internamente usa una instancia de la clase TreebankWordTokenizer (en la versión más reciente)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabrasNoticia = nltk.word_tokenize(texto)\n",
    "palabrasNoticia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debes saber que word_tokenize, no es la única función que permite realizar este trabajo. Puedes utilizar también casual_tokenize. Encuentra la diferencia =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Que', 'buena', 'pelicula', '.', 'Gracias', 'por', 'invitarme', ':)']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.casual.casual_tokenize(\"Que buena pelicula. Gracias por invitarme :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Que', 'buena', 'pelicula', '.', 'Gracias', 'por', 'invitarme', ':', ')']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"Que buena pelicula. Gracias por invitarme :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## Quitar palabras de parada\n",
    "\n",
    "Dependiendo del lenguaje existen palabras que tienden a repetirse mucho más que otras, estas generalmente son los articulos, las preposiciones, y las conjunciones. Estas palabras suelen ser perjudiciales al momento de analizar el texto, es por eso que se debe quitar las palabras de parada del vocabulario del problema. Tener en cuenta que NO hay una lista universal y exhaustiva de estas palabras. Cada lenguage e incluso tipo de problema puede tener su propia lista de palabras de parada.\n",
    "\n",
    "Puesto que estas listas pueden variar dependiendo de la librería o incluso entre versiones de la misma librería, incluir este paso puede dificultar la reproducción de los resultados en otros entornos. Veremos más adelante que hay otros mecanismos para lidiar con este tipo de palabras (TF, IDF).\n",
    "\n",
    "Finalmente, quitar alguna palabra de parada como los artículos podría cambiar completamente el significado de algunas palabras compuestas. Por ejemplo: \"La Paz\" -> \"Paz\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tienes problemas importando la libreria nltk, ejecuta este comando nltk.download('stopwords'). En el ejemplo se puede ver la  lista de palabras de parada que nos ofrece la libreria nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra lista de palabras de parada que podrías tomar en cuenta son las puntiaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes unir ambas listas para tener todo mejor organizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabrasParada = set( nltk.corpus.stopwords.words('spanish') + list(string.punctuation))\n",
    "palabrasParada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente solo queda eliminar las palabras de parada del texto de la noticia y listo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabrasNoticiaSinPalabrasDeParada = [palabra for palabra in palabrasNoticia if palabra not in palabrasParada]\n",
    "palabrasNoticiaSinPalabrasDeParada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## Otras tareas de limpieza que se pueden considerar\n",
    "\n",
    "- Unificar el case - string.lower().\n",
    "- Quitar acentos? probablemente no es buena idea si el contenido está en castellano\n",
    "- Procesar contracciones? I'm -> I am \n",
    "- Quitar caracteres expeciales - #@!\n",
    "- Quitar markup - Ej contenido HTML\n",
    "- Corregir texto -typos, noooo por favoooorrr\n",
    "\n",
    "Algunos enlaces de utilidad:\n",
    "- http://norvig.com/spell-correct.html\n",
    "- https://github.com/fsondej/autocorrect\n",
    "- https://github.com/MajorTal/DeepSpell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
